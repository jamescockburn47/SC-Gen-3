# multi_agent_rag_orchestrator.py

import asyncio
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import concurrent.futures
from pathlib import Path

from config import logger
from local_rag_pipeline import rag_session_manager
from mcp_rag_server import mcp_rag_server
from rag_config_optimizer import rag_optimizer

class TaskType(Enum):
    """Types of tasks that can be performed by different agents"""
    DOCUMENT_ANALYSIS = "document_analysis"
    LEGAL_EXTRACTION = "legal_extraction" 
    SUMMARY_GENERATION = "summary_generation"
    RISK_ASSESSMENT = "risk_assessment"
    ENTITY_EXTRACTION = "entity_extraction"
    DATE_EXTRACTION = "date_extraction"
    OBLIGATION_ANALYSIS = "obligation_analysis"
    COMPLIANCE_CHECK = "compliance_check"
    CROSS_REFERENCE = "cross_reference"
    SYNTHESIS = "synthesis"

@dataclass
class AgentTask:
    """Represents a task assigned to a specific agent"""
    task_id: str
    task_type: TaskType
    model_name: str
    priority: int
    context_chunks: List[Dict[str, Any]]
    query: str
    metadata: Dict[str, Any]
    dependencies: Optional[List[str]] = None  # Other task_ids this depends on

@dataclass
class AgentResult:
    """Result from an agent's task execution"""
    task_id: str
    task_type: TaskType
    model_name: str
    result: Dict[str, Any]
    execution_time: float
    confidence_score: float
    sources: List[Dict[str, Any]]
    metadata: Dict[str, Any]

class MultiAgentRAGOrchestrator:
    """
    Sophisticated multi-agent system that coordinates multiple models
    to work in parallel on different aspects of document analysis
    """
    
    def __init__(self, matter_id: str):
        self.matter_id = matter_id
        self.rag_pipeline = rag_session_manager.get_or_create_pipeline(matter_id)
        
        # Define agent specializations based on available models
        self.agent_specializations = {
            "deepseek-llm:67b": {
                "primary_tasks": [TaskType.DOCUMENT_ANALYSIS, TaskType.COMPLIANCE_CHECK, TaskType.SYNTHESIS],
                "secondary_tasks": [TaskType.RISK_ASSESSMENT, TaskType.CROSS_REFERENCE],
                "strengths": ["complex_reasoning", "comprehensive_analysis", "legal_synthesis"],
                "max_parallel": 1  # Large model, limit concurrency
            },
            "mixtral:latest": {
                "primary_tasks": [TaskType.LEGAL_EXTRACTION, TaskType.OBLIGATION_ANALYSIS, TaskType.RISK_ASSESSMENT],
                "secondary_tasks": [TaskType.DOCUMENT_ANALYSIS, TaskType.COMPLIANCE_CHECK],
                "strengths": ["legal_reasoning", "contract_analysis", "risk_identification"],
                "max_parallel": 2
            },
            "deepseek-llm:7b": {
                "primary_tasks": [TaskType.SUMMARY_GENERATION, TaskType.ENTITY_EXTRACTION],
                "secondary_tasks": [TaskType.DATE_EXTRACTION, TaskType.DOCUMENT_ANALYSIS],
                "strengths": ["fast_processing", "entity_recognition", "summarization"],
                "max_parallel": 3
            },
            "mistral:latest": {
                "primary_tasks": [TaskType.ENTITY_EXTRACTION, TaskType.DATE_EXTRACTION],
                "secondary_tasks": [TaskType.SUMMARY_GENERATION, TaskType.CROSS_REFERENCE],
                "strengths": ["pattern_recognition", "information_extraction", "structured_output"],
                "max_parallel": 3
            },
            "phi3:latest": {
                "primary_tasks": [TaskType.DATE_EXTRACTION, TaskType.ENTITY_EXTRACTION],
                "secondary_tasks": [TaskType.SUMMARY_GENERATION],
                "strengths": ["speed", "basic_extraction", "quick_analysis"],
                "max_parallel": 4
            }
        }
        
        # Track active tasks and agent availability
        self.active_tasks: Dict[str, AgentTask] = {}
        self.completed_tasks: Dict[str, AgentResult] = {}
        self.agent_availability: Dict[str, int] = {
            model: spec["max_parallel"] 
            for model, spec in self.agent_specializations.items()
        }
        
        # Task execution strategies
        self.task_strategies = {
            TaskType.DOCUMENT_ANALYSIS: self._create_document_analysis_strategy(),
            TaskType.LEGAL_EXTRACTION: self._create_legal_extraction_strategy(),
            TaskType.SUMMARY_GENERATION: self._create_summary_strategy(),
            TaskType.RISK_ASSESSMENT: self._create_risk_assessment_strategy(),
            TaskType.ENTITY_EXTRACTION: self._create_entity_extraction_strategy(),
            TaskType.DATE_EXTRACTION: self._create_date_extraction_strategy(),
            TaskType.OBLIGATION_ANALYSIS: self._create_obligation_analysis_strategy(),
            TaskType.COMPLIANCE_CHECK: self._create_compliance_check_strategy(),
            TaskType.CROSS_REFERENCE: self._create_cross_reference_strategy(),
            TaskType.SYNTHESIS: self._create_synthesis_strategy()
        }
    
    async def process_query(self, query: str, max_context_chunks: int = 10) -> Dict[str, Any]:
        """
        Process a query using multi-agent orchestration
        
        Returns comprehensive analysis from multiple agents working in parallel
        """
        start_time = datetime.now()
        
        # Step 1: Retrieve relevant document chunks
        relevant_chunks = self.rag_pipeline.search_documents(query, top_k=max_context_chunks)
        
        if not relevant_chunks:
            return {
                'answer': "No relevant documents found for your query.",
                'agents_used': [],
                'execution_time': 0,
                'confidence': 0
            }
        
        # Step 2: Analyze query and determine required tasks
        required_tasks = await self._analyze_query_requirements(query, relevant_chunks)
        
        # Step 3: Create and prioritize tasks
        task_plan = self._create_task_plan(required_tasks, relevant_chunks, query)
        
        # Step 4: Execute tasks in parallel with dependency management
        agent_results = await self._execute_task_plan(task_plan)
        
        # Step 5: Synthesize results from all agents
        final_result = await self._synthesize_results(query, agent_results, relevant_chunks)
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        return {
            'answer': final_result['synthesized_answer'],
            'agent_results': agent_results,
            'agents_used': list(set(result.model_name for result in agent_results.values())),
            'execution_time': execution_time,
            'confidence': final_result['confidence'],
            'sources': final_result['sources'],
            'metadata': final_result['metadata'],
            'task_breakdown': final_result['task_breakdown']
        }
    
    async def _analyze_query_requirements(self, query: str, chunks: List[Dict[str, Any]]) -> List[TaskType]:
        """Analyze query to determine what types of analysis are needed"""
        
        query_lower = query.lower()
        required_tasks = []
        
        # Always include basic document analysis
        required_tasks.append(TaskType.DOCUMENT_ANALYSIS)
        
        # Legal-specific triggers
        legal_keywords = ['contract', 'agreement', 'liability', 'obligation', 'clause', 'term', 'legal', 'law']
        if any(keyword in query_lower for keyword in legal_keywords):
            required_tasks.extend([TaskType.LEGAL_EXTRACTION, TaskType.OBLIGATION_ANALYSIS])
        
        # Risk-related triggers
        risk_keywords = ['risk', 'danger', 'problem', 'issue', 'concern', 'liability', 'exposure']
        if any(keyword in query_lower for keyword in risk_keywords):
            required_tasks.append(TaskType.RISK_ASSESSMENT)
        
        # Entity extraction triggers
        entity_keywords = ['who', 'company', 'person', 'organization', 'party', 'entity']
        if any(keyword in query_lower for keyword in entity_keywords):
            required_tasks.append(TaskType.ENTITY_EXTRACTION)
        
        # Date extraction triggers
        date_keywords = ['when', 'date', 'deadline', 'timeline', 'expire', 'due']
        if any(keyword in query_lower for keyword in date_keywords):
            required_tasks.append(TaskType.DATE_EXTRACTION)
        
        # Summary triggers
        summary_keywords = ['summary', 'summarize', 'overview', 'key points', 'main']
        if any(keyword in query_lower for keyword in summary_keywords):
            required_tasks.append(TaskType.SUMMARY_GENERATION)
        
        # Compliance triggers
        compliance_keywords = ['compliance', 'compliant', 'regulation', 'standard', 'requirement']
        if any(keyword in query_lower for keyword in compliance_keywords):
            required_tasks.append(TaskType.COMPLIANCE_CHECK)
        
        # Cross-reference triggers (for complex queries)
        if len(query.split()) > 15 or '?' in query and len(chunks) > 5:
            required_tasks.append(TaskType.CROSS_REFERENCE)
        
        # Always include synthesis for multi-task queries
        if len(required_tasks) > 2:
            required_tasks.append(TaskType.SYNTHESIS)
        
        return list(set(required_tasks))  # Remove duplicates
    
    def _create_task_plan(self, required_tasks: List[TaskType], chunks: List[Dict[str, Any]], query: str) -> List[AgentTask]:
        """Create detailed execution plan with task assignments and dependencies"""
        
        task_plan = []
        task_priorities = {
            TaskType.ENTITY_EXTRACTION: 1,  # Fast, no dependencies
            TaskType.DATE_EXTRACTION: 1,    # Fast, no dependencies
            TaskType.DOCUMENT_ANALYSIS: 2,  # Foundation for other tasks
            TaskType.LEGAL_EXTRACTION: 3,   # Depends on document analysis
            TaskType.SUMMARY_GENERATION: 3, # Can run parallel with legal extraction
            TaskType.RISK_ASSESSMENT: 4,    # Depends on legal extraction
            TaskType.OBLIGATION_ANALYSIS: 4, # Depends on legal extraction
            TaskType.COMPLIANCE_CHECK: 5,   # Depends on multiple analyses
            TaskType.CROSS_REFERENCE: 5,    # Depends on multiple analyses
            TaskType.SYNTHESIS: 6           # Final step, depends on all others
        }
        
        for i, task_type in enumerate(required_tasks):
            # Select best agent for this task
            best_agent = self._select_best_agent(task_type)
            
            # Determine dependencies
            dependencies = []
            if task_type == TaskType.RISK_ASSESSMENT:
                dependencies = [f"task_{j}" for j, t in enumerate(required_tasks) 
                              if t in [TaskType.DOCUMENT_ANALYSIS, TaskType.LEGAL_EXTRACTION]]
            elif task_type == TaskType.SYNTHESIS:
                dependencies = [f"task_{j}" for j, t in enumerate(required_tasks) if t != TaskType.SYNTHESIS]
            elif task_type in [TaskType.COMPLIANCE_CHECK, TaskType.CROSS_REFERENCE]:
                dependencies = [f"task_{j}" for j, t in enumerate(required_tasks) 
                              if t in [TaskType.DOCUMENT_ANALYSIS, TaskType.LEGAL_EXTRACTION]]
            
            task = AgentTask(
                task_id=f"task_{i}",
                task_type=task_type,
                model_name=best_agent,
                priority=task_priorities.get(task_type, 5),
                context_chunks=chunks,
                query=query,
                metadata={
                    'chunk_count': len(chunks),
                    'estimated_complexity': self._estimate_task_complexity(task_type, chunks)
                },
                dependencies=dependencies
            )
            
            task_plan.append(task)
        
        # Sort by priority (lower number = higher priority)
        task_plan.sort(key=lambda t: t.priority)
        
        return task_plan
    
    def _select_best_agent(self, task_type: TaskType) -> str:
        """Select the best available agent for a specific task type"""
        
        # Find agents that can handle this task type
        candidates = []
        for model, spec in self.agent_specializations.items():
            if task_type in spec["primary_tasks"]:
                candidates.append((model, 2))  # High priority for primary tasks
            elif task_type in spec["secondary_tasks"]:
                candidates.append((model, 1))  # Lower priority for secondary tasks
        
        if not candidates:
            # Fallback to most capable model
            return "mixtral:latest"
        
        # Sort by capability score and availability
        candidates.sort(key=lambda x: (x[1], self.agent_availability.get(x[0], 0)), reverse=True)
        
        return candidates[0][0]
    
    def _estimate_task_complexity(self, task_type: TaskType, chunks: List[Dict[str, Any]]) -> str:
        """Estimate task complexity based on type and context"""
        
        chunk_count = len(chunks)
        total_text_length = sum(len(chunk.get('text', '')) for chunk in chunks)
        
        complexity_weights = {
            TaskType.ENTITY_EXTRACTION: 1,
            TaskType.DATE_EXTRACTION: 1,
            TaskType.SUMMARY_GENERATION: 2,
            TaskType.DOCUMENT_ANALYSIS: 3,
            TaskType.LEGAL_EXTRACTION: 4,
            TaskType.RISK_ASSESSMENT: 4,
            TaskType.OBLIGATION_ANALYSIS: 4,
            TaskType.COMPLIANCE_CHECK: 5,
            TaskType.CROSS_REFERENCE: 5,
            TaskType.SYNTHESIS: 6
        }
        
        base_complexity = complexity_weights.get(task_type, 3)
        
        # Adjust based on content volume
        if chunk_count > 8 or total_text_length > 10000:
            complexity_score = base_complexity + 2
        elif chunk_count > 4 or total_text_length > 5000:
            complexity_score = base_complexity + 1
        else:
            complexity_score = base_complexity
        
        if complexity_score <= 2:
            return "low"
        elif complexity_score <= 4:
            return "medium"
        else:
            return "high"
    
    async def _execute_task_plan(self, task_plan: List[AgentTask]) -> Dict[str, AgentResult]:
        """Execute task plan with parallel processing and dependency management"""
        
        completed_results = {}
        remaining_tasks = {task.task_id: task for task in task_plan}
        
        while remaining_tasks:
            # Find tasks that can be executed (dependencies satisfied)
            ready_tasks = []
            for task_id, task in remaining_tasks.items():
                if not task.dependencies or all(dep in completed_results for dep in task.dependencies):
                    ready_tasks.append(task)
            
            if not ready_tasks:
                logger.error("Circular dependency detected in task plan")
                break
            
            # Execute ready tasks in parallel (respecting agent capacity)
            execution_futures = []
            for task in ready_tasks:
                if self.agent_availability[task.model_name] > 0:
                    self.agent_availability[task.model_name] -= 1
                    future = asyncio.create_task(self._execute_single_task(task))
                    execution_futures.append((task, future))
            
            # Wait for completion and collect results
            for task, future in execution_futures:
                try:
                    result = await future
                    completed_results[task.task_id] = result
                    remaining_tasks.pop(task.task_id)
                    self.agent_availability[task.model_name] += 1
                except Exception as e:
                    logger.error(f"Task {task.task_id} failed: {e}")
                    # Create error result
                    completed_results[task.task_id] = AgentResult(
                        task_id=task.task_id,
                        task_type=task.task_type,
                        model_name=task.model_name,
                        result={'error': str(e)},
                        execution_time=0,
                        confidence_score=0,
                        sources=[],
                        metadata={'error': True}
                    )
                    remaining_tasks.pop(task.task_id)
                    self.agent_availability[task.model_name] += 1
        
        return completed_results
    
    async def _execute_single_task(self, task: AgentTask) -> AgentResult:
        """Execute a single task using the assigned agent"""
        
        start_time = datetime.now()
        
        # Get task-specific strategy
        strategy = self.task_strategies.get(task.task_type)
        if not strategy:
            raise ValueError(f"No strategy defined for task type {task.task_type}")
        
        # Build task-specific prompt
        prompt = strategy['prompt_template'].format(
            query=task.query,
            context=self._format_context_for_task(task.context_chunks, task.task_type),
            **strategy.get('additional_params', {})
        )
        
        # Execute with assigned model
        rag_result = await self.rag_pipeline.generate_rag_answer(
            query=prompt,
            model_name=task.model_name,
            max_context_chunks=len(task.context_chunks),
            temperature=strategy.get('temperature', 0.1)
        )
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # Parse and structure result based on task type
        structured_result = strategy['result_parser'](rag_result['answer'])
        
        # Calculate confidence score
        confidence = self._calculate_confidence(rag_result, task.task_type)
        
        return AgentResult(
            task_id=task.task_id,
            task_type=task.task_type,
            model_name=task.model_name,
            result=structured_result,
            execution_time=execution_time,
            confidence_score=confidence,
            sources=rag_result.get('sources', []),
            metadata={
                'prompt_tokens': rag_result.get('prompt_tokens', 0),
                'response_tokens': rag_result.get('response_tokens', 0),
                'complexity': task.metadata.get('estimated_complexity', 'medium')
            }
        )
    
    def _format_context_for_task(self, chunks: List[Dict[str, Any]], task_type: TaskType) -> str:
        """Format context chunks specifically for the task type"""
        
        if task_type in [TaskType.ENTITY_EXTRACTION, TaskType.DATE_EXTRACTION]:
            # For extraction tasks, provide more structured context
            context_parts = []
            for i, chunk in enumerate(chunks):
                context_parts.append(f"[Document {i+1}: {chunk.get('document_info', {}).get('filename', 'Unknown')}]\n{chunk['text']}")
        else:
            # Standard context formatting
            context_parts = [f"[Source {i+1}] {chunk['text']}" for i, chunk in enumerate(chunks)]
        
        return "\n\n".join(context_parts)
    
    def _calculate_confidence(self, rag_result: Dict[str, Any], task_type: TaskType) -> float:
        """Calculate confidence score based on result quality and task type"""
        
        base_confidence = 0.7
        
        # Adjust based on sources
        sources = rag_result.get('sources', [])
        if sources:
            avg_similarity = sum(s.get('similarity_score', 0) for s in sources) / len(sources)
            base_confidence += (avg_similarity - 0.5) * 0.4
        
        # Adjust based on task type complexity
        complexity_adjustments = {
            TaskType.ENTITY_EXTRACTION: 0.1,
            TaskType.DATE_EXTRACTION: 0.1,
            TaskType.SUMMARY_GENERATION: 0.05,
            TaskType.DOCUMENT_ANALYSIS: 0.0,
            TaskType.LEGAL_EXTRACTION: -0.05,
            TaskType.RISK_ASSESSMENT: -0.1,
            TaskType.SYNTHESIS: -0.15
        }
        
        base_confidence += complexity_adjustments.get(task_type, 0)
        
        return max(0.0, min(1.0, base_confidence))
    
    async def _synthesize_results(self, query: str, agent_results: Dict[str, AgentResult], chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Synthesize results from multiple agents into a comprehensive answer"""
        
        # If we have a synthesis task result, use it as the primary answer
        synthesis_result = None
        for result in agent_results.values():
            if result.task_type == TaskType.SYNTHESIS:
                synthesis_result = result
                break
        
        if synthesis_result and not synthesis_result.metadata.get('error'):
            primary_answer = synthesis_result.result.get('synthesized_answer', '')
        else:
            # Fallback: use document analysis result or create basic synthesis
            doc_analysis_result = None
            for result in agent_results.values():
                if result.task_type == TaskType.DOCUMENT_ANALYSIS:
                    doc_analysis_result = result
                    break
            
            if doc_analysis_result:
                primary_answer = doc_analysis_result.result.get('analysis', '')
            else:
                primary_answer = "Analysis completed with multiple agents, but synthesis failed."
        
        # Compile comprehensive metadata
        all_sources = []
        total_confidence = 0
        successful_agents = 0
        
        for result in agent_results.values():
            if not result.metadata.get('error'):
                all_sources.extend(result.sources)
                total_confidence += result.confidence_score
                successful_agents += 1
        
        avg_confidence = total_confidence / successful_agents if successful_agents > 0 else 0
        
        # Create task breakdown for transparency
        task_breakdown = {}
        for result in agent_results.values():
            task_breakdown[result.task_type.value] = {
                'model': result.model_name,
                'confidence': result.confidence_score,
                'execution_time': result.execution_time,
                'success': not result.metadata.get('error', False),
                'key_findings': self._extract_key_findings(result)
            }
        
        return {
            'synthesized_answer': primary_answer,
            'confidence': avg_confidence,
            'sources': list({s['chunk_id']: s for s in all_sources}.values()),  # Deduplicate
            'metadata': {
                'total_agents_used': len(agent_results),
                'successful_agents': successful_agents,
                'failed_agents': len(agent_results) - successful_agents,
                'total_execution_time': sum(r.execution_time for r in agent_results.values()),
                'query_complexity': len(agent_results),
                'synthesis_method': 'multi_agent_orchestration'
            },
            'task_breakdown': task_breakdown
        }
    
    def _extract_key_findings(self, result: AgentResult) -> List[str]:
        """Extract key findings from an agent result"""
        
        if result.metadata.get('error'):
            return ['Task failed']
        
        # Extract based on task type
        if result.task_type == TaskType.ENTITY_EXTRACTION:
            entities = result.result.get('entities', [])
            return [f"Found {len(entities)} entities"] if entities else ["No entities found"]
        
        elif result.task_type == TaskType.DATE_EXTRACTION:
            dates = result.result.get('dates', [])
            return [f"Found {len(dates)} important dates"] if dates else ["No dates found"]
        
        elif result.task_type == TaskType.RISK_ASSESSMENT:
            risks = result.result.get('risks', [])
            return [f"Identified {len(risks)} risks"] if risks else ["No risks identified"]
        
        elif result.task_type == TaskType.SUMMARY_GENERATION:
            summary = result.result.get('summary', '')
            return [f"Generated summary ({len(summary)} chars)"] if summary else ["No summary generated"]
        
        else:
            # Generic extraction
            return ["Analysis completed"]
    
    # Task Strategy Definitions
    def _create_document_analysis_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """You are analyzing legal documents. ONLY use information explicitly stated in the provided documents.

STRICT REQUIREMENTS:
- Extract facts ONLY from the provided documents
- Use exact citations [Source 1], [Source 2] etc.
- If information is not in documents, state "This information is not provided in the documents"
- Do NOT use placeholder text like "[DATE]", "[Source X, Page XX]", or "[OR: entity name]"
- Quote directly when possible
- Do NOT make assumptions or inferences beyond what is written

QUERY: {query}

DOCUMENTS:
{context}

ANALYSIS (based ONLY on the provided documents):""",
            'temperature': 0.0,
            'result_parser': lambda text: {'analysis': text.strip()},
            'additional_params': {}
        }
    
    def _create_legal_extraction_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Extract legal information from the following documents relevant to: {query}

{context}

Identify and extract:
1. Legal clauses and provisions
2. Rights and obligations
3. Legal references (statutes, cases, regulations)
4. Contractual terms and conditions
5. Legal definitions and interpretations

Provide specific citations and quotes from the source documents.""",
            'temperature': 0.05,
            'result_parser': lambda text: {'legal_content': text.strip()},
            'additional_params': {}
        }
    
    def _create_summary_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Create a concise but comprehensive summary of the following documents in relation to: {query}

{context}

Provide:
1. Executive summary (2-3 sentences)
2. Key points (bullet format)
3. Main conclusions
4. Relevant details specific to the query

Keep the summary focused and actionable.""",
            'temperature': 0.15,
            'result_parser': lambda text: {'summary': text.strip()},
            'additional_params': {}
        }
    
    def _create_risk_assessment_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Assess risks and potential issues in the following documents related to: {query}

{context}

Identify:
1. Legal risks and liabilities
2. Financial risks
3. Operational risks
4. Compliance risks
5. Mitigation strategies mentioned

Rate each risk as HIGH/MEDIUM/LOW and provide specific evidence from the documents.""",
            'temperature': 0.1,
            'result_parser': lambda text: {'risk_assessment': text.strip()},
            'additional_params': {}
        }
    
    def _create_entity_extraction_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Extract all entities (people, organizations, locations, etc.) from the following documents relevant to: {query}

{context}

For each entity, provide:
1. Entity name
2. Entity type (Person, Organization, Location, etc.)
3. Role or relevance
4. Source document reference

Format as a structured list.""",
            'temperature': 0.05,
            'result_parser': self._parse_entity_extraction,
            'additional_params': {}
        }
    
    def _create_date_extraction_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Extract all important dates and deadlines from the following documents related to: {query}

{context}

For each date, provide:
1. The specific date
2. What the date represents (deadline, effective date, expiration, etc.)
3. Importance level (Critical/Important/Minor)
4. Source document reference

Format as a chronological list.""",
            'temperature': 0.05,
            'result_parser': self._parse_date_extraction,
            'additional_params': {}
        }
    
    def _create_obligation_analysis_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Analyze obligations, duties, and responsibilities in the following documents related to: {query}

{context}

Identify:
1. Parties with obligations
2. Specific obligations and duties
3. Deadlines and timelines
4. Consequences of non-compliance
5. Performance requirements

Organize by party and provide source citations.""",
            'temperature': 0.1,
            'result_parser': lambda text: {'obligations': text.strip()},
            'additional_params': {}
        }
    
    def _create_compliance_check_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Check compliance and regulatory requirements in the following documents related to: {query}

{context}

Evaluate:
1. Regulatory requirements mentioned
2. Compliance obligations
3. Standards and certifications required
4. Audit and reporting requirements
5. Potential compliance gaps

Provide recommendations for ensuring compliance.""",
            'temperature': 0.1,
            'result_parser': lambda text: {'compliance_analysis': text.strip()},
            'additional_params': {}
        }
    
    def _create_cross_reference_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """Cross-reference and connect information across the following documents related to: {query}

{context}

Identify:
1. Connections between different documents
2. Contradictions or inconsistencies
3. Supporting or conflicting information
4. References between documents
5. Gaps in information

Provide a comprehensive cross-analysis.""",
            'temperature': 0.15,
            'result_parser': lambda text: {'cross_reference': text.strip()},
            'additional_params': {}
        }
    
    def _create_synthesis_strategy(self) -> Dict[str, Any]:
        return {
            'prompt_template': """You must synthesize information ONLY from the provided documents to answer: {query}

CRITICAL RULES:
- Use ONLY facts explicitly stated in the documents below
- Cite sources using [Source 1], [Source 2] format
- If information is missing, state "This information is not provided in the documents"
- NO placeholder text like "[DATE]", "[Source X, Page XX]", "[OR: ...]"
- NO assumptions or inferences beyond document content
- Quote directly from documents when possible

DOCUMENTS:
{context}

COMPREHENSIVE ANSWER (based ONLY on provided documents):""",
            'temperature': 0.0,
            'result_parser': lambda text: {'synthesized_answer': text.strip()},
            'additional_params': {}
        }
    
    def _parse_entity_extraction(self, text: str) -> Dict[str, Any]:
        """Parse entity extraction results into structured format"""
        # Simple parsing - could be enhanced with NLP
        entities = []
        lines = text.strip().split('\n')
        
        for line in lines:
            if line.strip() and not line.strip().startswith('#'):
                entities.append(line.strip())
        
        return {'entities': entities, 'entity_count': len(entities)}
    
    def _parse_date_extraction(self, text: str) -> Dict[str, Any]:
        """Parse date extraction results into structured format"""
        dates = []
        lines = text.strip().split('\n')
        
        for line in lines:
            if line.strip() and not line.strip().startswith('#'):
                dates.append(line.strip())
        
        return {'dates': dates, 'date_count': len(dates)}


# Global orchestrator instances per matter
_orchestrator_instances: Dict[str, MultiAgentRAGOrchestrator] = {}

def get_orchestrator(matter_id: str) -> MultiAgentRAGOrchestrator:
    """Get or create orchestrator instance for a matter"""
    if matter_id not in _orchestrator_instances:
        _orchestrator_instances[matter_id] = MultiAgentRAGOrchestrator(matter_id)
    return _orchestrator_instances[matter_id] 